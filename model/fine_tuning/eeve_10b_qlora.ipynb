{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEVE-10.8B QLoRA Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U accelerate\n",
    "!pip install -U peft\n",
    "!pip install -U trl\n",
    "!pip install -U typing_extensions\n",
    "!pip install -U torch\n",
    "!pip install -U datasets\n",
    "!pip install -U korouge_score\n",
    "!pip install -U konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from korouge_score import rouge_scorer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"MartinusChoi/FinPilot\", split=\"train\")\n",
    "test_dataset = load_dataset(\"MartinusChoi/FinPilot\", split=\"test\")\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"prompt\", \"text\")\n",
    "test_dataset = test_dataset.rename_column(\"prompt\", \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set PLM Into QLoRA Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     # Load model in 4bit precision\n",
    "    bnb_4bit_quant_type='nf4',             # Pre-trained model has to be quantization in 4bit nf type\n",
    "    bnb_4bit_use_double_quant=True,        # Use double-qauntization of QLoRA\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Pre-trained model has to be loaded in BF16 dtype\n",
    ")\n",
    "\n",
    "plm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=True,\n",
    "    quantization_config=bnb_config,        # Use bitsandbytes config\n",
    "    device_map='auto',                     # auto : HF Accelerate determines which GPU to allocate for each layer of the model.\n",
    "    trust_remote_code=True                 # Setting for use EEVE model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('yanolja/EEVE-Korean-Instruct-10.8B-v1.0', token=True,)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flm = prepare_model_for_kbit_training(plm)\n",
    "\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_rank = 32\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_rank,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj']\n",
    ")\n",
    "\n",
    "flm=get_peft_model(flm, peft_config)\n",
    "flm.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments =  transformers.TrainingArguments(\n",
    "    output_dir = './train_output',\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 2,                                               # 배치 크기가 줄어들면 기울기 누적 단계가 2배 증가\n",
    "    optim = 'paged_adamw_32bit',                                                   # 더 나은 메모리 관리를 위해 페이징을 활성화\n",
    "    save_strategy='steps',                                                         # 학습 중에 채택할 체크포인트 save strategy\n",
    "    save_steps = 10,                                                               # 두 개의 체크포인트가 저장되기 전의 업데이트 단계 수\n",
    "    logging_steps = 10,                                                            # 두 로그 사이의 업데이트 단계 수\n",
    "    learning_rate = 2e-4,                                                          # AdamW 최적화 프로그램의 학습률\n",
    "    max_grad_norm = 0.3,                                                           # 최대 그라데이션 표준(gradient clipping)\n",
    "    max_steps = 60,                                                                # 60 단계 동안 학습\n",
    "    warmup_ratio = 0.03,                                                           # 0 에서 learning_rate 까지 선형 준비에 사용되는 단계 수\n",
    "    lr_scheduler_type = 'cosine',                                                  # 학습률 스케줄러\n",
    "    report_to = 'none',                                                            # You can find your API key in your browser here: https://wandb.ai/authorize\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\n",
    "  # Dict for store 'rouge' score per predicted summary\n",
    "  scores = {\n",
    "      \"rouge1\": [],\n",
    "      \"rouge2\": [],\n",
    "      \"rougeL\": [],\n",
    "      \"rougeLsum\": []\n",
    "  }\n",
    "\n",
    "  # 한국어 rouge score 계산 인스턴스 생성\n",
    "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'])\n",
    "\n",
    "  # 모델의 예측 정보 파싱\n",
    "  predictions, labels = eval_pred\n",
    "\n",
    "  # 모델 예측의 각 토큰의 대한 logit을 토큰으로 변환\n",
    "  # prediction shape : (batch, max_length, 51200)\n",
    "  # 마지막 차원(51200)은 어휘 사전 크기 만큼의 logit\n",
    "  # np.argmax() 를 활용해 가장 확률 높은 token id로 변환\n",
    "  predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "  # 모델 예측 token id sequence -> 텍스트\n",
    "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "  # 전체 batch 크기 만큼의 rouge score 종합\n",
    "  for pred, ref in zip(decoded_preds, decoded_labels):\n",
    "      score = scorer.score(ref, pred)\n",
    "      for rouge_type in scores.keys():\n",
    "          scores[rouge_type].append(score[rouge_type].fmeasure)\n",
    "\n",
    "  # batch만큼의 평균 rouge score 계산\n",
    "  avg_scores = {rouge_type: sum(score_list) / len(score_list) * 100 for rouge_type, score_list in scores.items()}\n",
    "\n",
    "  return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=flm,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args = training_arguments,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if 'norm' in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flm.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare between PLM and FLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 입력 텍스트\n",
    "input_text = \"삼성전자는 메모리 반도체 분야에서 세계적인 리더로 자리잡고 있습니다. 최근 HBM(고대역폭 메모리) 기술 경쟁력을 회복하며 시장에서의 입지를 강화하고 있습니다. 이는 AI 및 데이터 센터 수요 증가에 따른 것으로, 삼성전자는 이러한 수요에 대응하기 위해 기술 개발에 박차를 가하고 있습니다. 이 텍스트를 요약해줘\"\n",
    "\n",
    "# FLM 모델 사용\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# 입력 텐서를 GPU로 이동\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# 모델 실행\n",
    "output = flm.generate(**inputs, max_length=200)\n",
    "result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Output:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 입력 텍스트\n",
    "input_text = \"삼성전자는 메모리 반도체 분야에서 세계적인 리더로 자리잡고 있습니다. 최근 HBM(고대역폭 메모리) 기술 경쟁력을 회복하며 시장에서의 입지를 강화하고 있습니다. 이는 AI 및 데이터 센터 수요 증가에 따른 것으로, 삼성전자는 이러한 수요에 대응하기 위해 기술 개발에 박차를 가하고 있습니다. 이 텍스트를 확장해줘\"\n",
    "\n",
    "# plm 모델 사용\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# 입력 텐서를 GPU로 이동\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "output = plm.generate(**inputs, max_length=1024)\n",
    "result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Output:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
